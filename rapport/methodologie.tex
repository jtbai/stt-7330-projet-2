\section{Méthodologie}

\subsection{Présentation du jeu de données}
Le jeu de données utilisé dans ce travail est composés de différentes statistiques sur les match officiels de l'association des professionnels de tennis depuis 1968. Il est distribuées par Jeff Sackmann via github. 

Le jeu de données comprend la surface sur laquelle un match de tennis a été joué. C'est cette variable que la machine apprenante devra prédire.  Il s'agit d'une variable catégorielle pouvant prendre 3 valeurs  soit: Clay, Hard et Grass. Il y a environ 50 autres variables qui sont utilisées pour tenter d'expliquer la variable à prédire. Ces valeurs représentent soit des caractéristiques sur le tournois (Niveau, endoit), sur les joueurs (age, pays d'orgine, main forte) ou sur la partie (score final, nombre de ACE pour le gagnant, longueur de la partie en minute).


\subsection{Prétraîtement des données}
L'information contenue dans le jeu de données étaient très clairsemée. Ce fléau de la dimentionnalité implique que l'apprentissage se faisait de façon sous optimale. Les modèles finaux peuvent ne pas être optimaux ou fiable. Afin de palier à ce problème nous avons décidé d'améliorer la qualité du jeu de données. Les différentes étapes ont été de nettoyager des données, l'identification et le traitement des valeurs manquantes, intégration et transformation des variables, et enfin la réduction de la dimensionnalité.

- Nettoyage des données
Afin d'obtenir des données pouvant être utilisées par nos algorithmes, nous avons effectuée quelques transformation du jeu de données de base. Premièrement, nous avons enlevée toutes les parties ayant comme surface "carpet" car il n'y avait en avait pas suffisament. Les données dépendantes de la surface du jeu ont aussi été enlevée. En effet, dans le cas où ces données auraient été conservées, la machine apprenante aurait pu tout simplement utiliser ces variables et le classification aurait été triviale. Finalement, les variables considérées non informatives (telles le nom du joueur, la date du match, le numéro du match) ont été enlevée.

- intégration et transformation des variables (\italic{Feature engineering}) 
Cette étape est cruciale car elle permet de créer d'autres variables non linairement dépendantes aux autre permettant d'avoir des information additionnelle qui n'existaient pas à priori dans le jeu de données. Le choix de ces variables est basé sur l'expérience et l'intuition du domaine et du jeu de données.
Les variables qui ont été créees sont des ratios entre différents éléments du jeu de données.

- Traitement des valeurs manquantes
Il n'y avait pas réellement de valeur manquante dans le jeu de données initial. Nous avons cependant imputé des valeurs de 0 pour les ratios conçus précedemment pour lesquels la valeur etait infini (par une division par zero)

- Réduction de la dimensionalité
Tel que mentionné ci-dessus, le jeu de données comprennait environ une cinquantaine de variables explicatives servant à entrainer un modèle. Ceci peut causer une problème si certaines de ces variables sont non-informatives ou redondantes. Il a été choisi de faire une analyse par composantes principales (PCA) du jeu de données. Celle-ci permet d'obtenir une représentation réduite conservant une grande partie de l'information, produisant les mêmes (ou presque) résultats analytiques. L'ACP a réduit le nombre des variables utilisées à 7, tout en maintenant une quatité de variance expliquée de 80 \%. 

\subsection{Classifieurs utilisés}

Les algorithmes d'apprentissage automatique utilisés pour le projets varient en complexité. Nous désirions tester différents niveaux de complexité afin de déterminer si une augmentation de la complexité était justifiée relativement à la qualité des prédictions. Nous avons librement sous divisés les algorithme en 3 catégories distinctes. 

La première catégorie d'algoritme les plus simple ne demande qu'une compréhension du modèle afin de l'utiliser. Les modèles comme 
- le classifieur de naïf de Bayes, 
- le classifieur régression logistique, 
- l'analyse discriminante linéaire et 
- l'analyse discriminante quadratique 

font partie de cette première catégorie. Nous considérons qu'ils sont simple car ils sont utilisables directement avec une fonction R. 

La deuxième catégorie d'algoritmes sont un peu plus complexes et demandent un ajustement de leur hyper-paramètres. Les modèles comme 
- le classifieur par arbre, 
- le classifieur forêt aléatoire et 
- le classifieur SVM 

font parti de cette deuxième catégories. La méthodologie de la sélection des hyper-paramètres est présentée dans une section ultérieure.

Finalement, la troisième catégorie d'algoritme sont les plus complexe et demandent l'utilisation de l'expérience et l'intuition du statisticien. Les modèles comme 
- les réseaux de neuronnes et 
- le modèle par emsemble 

font parti de cette troisième catégorie. L'utilisation et l'architecture de ces modèles seront présentés dans une section ultérieure

\subsection{Ajustement des hyper-paramètres}

L'ajustement des hyper-paramètre a été faite par une recherche exhaustive. La technique de recherche par quadrillage (grid-search). Afin de ne pas surajuster les modèles lors de cette recherche, nous avons utilisés la validation croisée avec 10 plis.

La recherche par quadrillage est une technique permettant de trouver les meilleurs hyper-paramètres en essayant plusieurs combinaisons entre les différentes valeurs de chacun des hyper-paramètres. Comme notre connaissance du jeu de données était plutôt limité, nous avons décidés de considérer une large gamme de valeurs et de faire une recherche exhaustive pour chacune des combinaison d'hyper-parametre possibles.


\subsubsection{Hyper-paramètres du modèle par arbre}
Pour le modèle par arbre, les hyper-paramètre utilisés sont maxdepth et minsplit. 

L'hyper-paramètre maxdepth controle la profondeur maximale de l'arbre. La profondeur maximale de l'arbre est en quelque sorte le nombre de divisions pouvant être faites dans le jeu de données. En terme du compromis biais variance, une grande profondeur augmente la variance et réduit le biais. Nous avons considéré les valeur 1, 3, 5 et 10

L'hyper-paramètre minsplit controle le nombre de point de données minimum dans un noeud pour qu'il soit divisé. Ainsi plus ce paramètre est grand, plus l'arbre aura des feuille fournies. En terme du compromis biais variance, un paramètre minsplit élevé donnera à l'arbre une variance faible mais un biais élevé. Nous avons considéré les valeurs de 2, 5, 8, 10, 15 et 20


\subsubsection{Hyper-paramètres du modèle de forêt aléatoire}
Pour le modèle de forêt aléatoire, les hyper-paramètres utilisés sont mtry et ntree. 

L'hyper-paramètre mtry controle le nombre de variable explicative sélectionnées aléatoirement lors de la création d'un noeud.  En terme du compromis biais variance, l'augmentation de mtry causera un biais plus petit, mais aura une variance plus grande.  Nous avons considéré les valeur 4, 8, 12 et 16.

L'hyper-paramètre ntree controle le nombre d'arbre générés aléatoirement pour créer le classificateur. Il est important d'avoir un nombre suffisament grand afin de faire baisser la variance et ainsi atteindre le compromis biais variance optimal. Nous avons considéré les valeurs de 700, 1000 et 2000. 



\subsubsection{Hyper-paramètres du modèle SVM}
Pour le modèle de forêt aléatoire, les hyper-paramètres utilisés sont cost et epsilon. 

L'hyper-paramètre cost controle le la fréquence et la sévérité des violation de la marge.  En terme du compromis biais variance, l'augmentation de cost causera un biais plus grand, mais aura une variance plus petite. Nous avons considéré les valeur 0.90, 0.95, 0.99 et 1.

L'hyper-paramètre epsilon controle (........_). Nous avons considéré les valeurs de 0.1,0.01 et.001. 


\subsection{Ajustement des modèles complexes}

\subsubsection{Modèles par ensembles}

\subsubsection{Réseau de neuronnes}

Finalement, nous avons fait la classification en entraînant un réseau de neurones profonds. Étant donné que le but premier de ce projet n'est pas de construire un réseau de neurones en soi, nous avons choisi une architecture relativement simple. Nous avons donc choisi un réseau à propagation avant avec 2 couches cachées et une couche de sortie à 3 neurones (un pour chacune des réponses). Puisque c'est un problème de classification à multiple classes, nous avons choisi comme mesure de perte la \textit{categorical-crossentropy} et comme métrique à optimiser la précision. La librairie utilisée est celle de \texttt{keras}.

Considérant la nature de ce type de méthode, il est important de mentionner le fait que nous n'avons pas structuré les données de la même façon pour ce type de modèle. En effet, celui-ci est mieux adapté pour trouver des interactions et faire une sélection de variables. Ainsi, nous n'avons pas construit de variables précises (comme dans la méthode classiques), nous avons plutôt donné en entrée les données brutes au modèle.

