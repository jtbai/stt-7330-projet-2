\section{Résultats}
Maintenant que les différents modèles à entraîner ainsi que leurs hyperparamètres respectifs sont défini, il ne reste plus qu'à évaluer leur performance et choisir le modèle final. Une fois le modèle final déterminé, il ne restera plus qu'à faire une brève analyse de celui-ci avec une dernière évaluation de sa performance sur les données de validation, qui n'ont toujours pas été vues par le modèle.  


\subsection{Évaluation des performances}
Dans le but d'orienter notre choix de modèle, nous devons définir des mesures de performances qui permettront de comparer les modèles entres eux. \\

Dans les tâches de classification, les métriques que nous utilisons mettent l'emphase sur différents objectifs de classification. On pourrait vouloir par exemple mesurer la capacité du classifieur à prédire une bonne classe, la capacité de ne pas faire de fausses prédictions et/ou de ne jamais manquer de cas positifs. Ces mesures sont utilisées dans le cas binaire, où le classifier doit choisir soit une classe ou l'autre. Dans le  problème actuel, l'utilisation de 3 classes impose qu'une transformation vers plusieurs problèmes binaires soit faite afin de pouvoir calculer ces métriques. \\

Afin de transformer le problème de classification multiclasse en classification binaire, chaque classe sera prise indépendamment et sera confrontée aux 2 autres agrégées ensemble (méthode tous contre un). Cela revient à faire 3 classifications binaires. Une stratégie d'aggrégation doit être mise en place pour sommariser les statistiques des 3 classifiers. \\ Une stratégie serait d'attribuer un poids identique à chacunes des métriques générées par les 3 classifieurs binaires. Ce sont les métrique de type macro qu'on utilise lorsqu'une classe minoritaire doit être considérée aussi importante que les autres classes. \\ Une autre stratégie serait d'attribuer des poids proportionnels à la quantité de données appartenant à chacune des classes aux métriques générées par les 3 classifieurs binaires. Ce sont les métriques de type micro qu'on utilise lorsque qu'aucune classe n'a plus d'importance qu'une autre - ou qu'il n'y a pas d'incitatif à prédire une classe plus qu'une autre. \\

Dans le cadre de ce problème, nous utiliserons les métrique de type micro car il n'y a pas de classe ayant besoin d'une attention accrue. Formellement, pour chaque classe $l$, les métriques utilisées sont: \\

- Exactitude (Taux de bonnes détections): l'exactitude est le pouvoirédu classificateur à prédire la bonne classe pour une observation donnée.\\
\begin{center}
	$\textrm{Exactitude} = \frac{\sum_{i=1}^{l} \frac{tp_i +tn_i}{tp_i + fn_i + fp_i+ tn_i}}{l}$
\end{center}
	
- Taux d'erreur: le taux d'erreur est le taux auquel le classificateur ne prédit pas la bonne classe pour une observation données.\\ 
\begin{center}
	$\textrm{Taux d'erreur} = \frac{\sum_{i=1}^{l} \frac{fp_i +fn_i}{tp_i + fn_i + fp_i+ tn_i}}{l}$
\end{center}

- Précision : la précision la capacité d'un classificateur à ne trouver que les données d'une classe en particulier. (Minimise les faux positifs) \\
\begin{center}
	$\textrm{Précision} = \frac{\sum_{i=1}^{l} tp_i}{\sum_{i=1}^{l} (tp_i + fp_i)}$
\end{center}

- Rappel (sensibilité): le rappel est la capacité du classificateur à bien trouver toutes les données d'une classe en particulier. (Maximise les vrai positifs)\\
\begin{center}
	$\textrm{Rappel} = \frac{\sum_{i=1}^{l} tp_i}{\sum_{i=1}^{l} (tp_i + fn_i)}$
\end{center}

- Score F: le score F est la moyenne harmonique du précision et du rappel. Elle sert de mesure agrégée sur laquelle prendre une décision. classe.\\	
\begin{center}
	$\textrm{Score F} = \frac{2 * \textrm{Précision} * \textrm{Rappel}}{\textrm{Précision} + \textrm{Rappel}}$ 
\end{center}

\subsection{Choix du modèle}

Dans le tableau \ref{tab:perfo_test}, nous présenterons l'exactitude et le score F pour chacun des classifieurs pour le jeu de données complet et à dimentionnalité réduite. Le choix du modèle final sera fait en considérant ces deux statistiques. \\

Comme le Score F agrégège les concepts de précision et rappel, il a été choisi de ne pas présenter ces deux dernières. En effet, le but de notre classification est de donner la meilleure prédiction, peut importe la classe et il n'y a pas d'erreur plus grave qu'une autre. L'exactitude ou le Score F1 sont deux métriques appropriées pour l'évaluation de notre classifieur. Dans le cas où il y aurait une grande différence entre le Score F1 et l'exactitude, il pourrait être intéressant de bien comprendre d'ou vient cet écart en analyant les ratios intermédiaires de précision et de rappel.

\rowcolors{2}{white}{white}\rowcolors{2}{gray!6}{white}
\begin{table}[H]
	
	\caption{\label{tab:perfo_test}Performances des différents modèles sur le jeu de données de test selon les différentes mesures de performance.}
	\centering
	\resizebox{\linewidth}{!}{\begin{tabular}[t]{lclcl}
			\hiderowcolors
			\toprule
			Modèles & Exactitude & F-Score & Exactitude avec PCA & F-Score avec PCA\\
			\midrule
			\showrowcolors
			Réseau de neurones & 0.78 & 0.84 & NA & NA\\
			Ensemble & 0.76 & 0.82 & 0.75 & 0.81\\
			Multinomial & 0.76 & 0.82 & 0.75 & 0.81\\
			LDA & 0.76 & 0.82 & 0.75 & 0.81\\
			Forêt aléatoire & 0.76 & 0.82 & NA & NA\\
			\addlinespace
			Arbre de décision & 0.76 & 0.82 & NA & NA\\
			Bayes & 0.75 & 0.81 & 0.74 & 0.80\\
			QDA & 0.72 & 0.79 & 0.74 & 0.80\\
			Bayes & 0.71 & 0.78 & 0.74 & 0.81\\
			SVM Gaussien & 0.70 & 0.78 & NA & NA\\
			\bottomrule
	\end{tabular}}
\end{table}


\subsection{Présentation du modèle final}

Note: Ici on roule le modele sur les donnees de validation (split group = 3), on fait le lien avec le classifieur au hasard pour avoir une idée de la performance (benchmark) et on pourrait tenter de faire une genre d'interpreation (quelles vairables parlent, quelles parlent pas, ...)

\rowcolors{2}{white}{white}
\rowcolors{2}{gray!6}{white}
\begin{table}[H]
	
	\caption{\label{tab:matrice_confusion}Matrice de confusion pour le modèle final.}
	\centering
	\begin{tabular}[t]{llcc}
		\hiderowcolors
		\toprule
		& Hard & Clay & Grass\\
		\midrule
		\showrowcolors
		Hard & 16566 & 5468 & 2738\\
		Clay & 1389 & 5375 & 173\\
		Grass & 298 & 138 & 740\\
		\bottomrule
	\end{tabular}
\end{table}

