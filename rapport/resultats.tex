\section{Résultats}
Maintenant que les différents modèles à entraîner ainsi que leurs hyperparamètres respectifs sont définis, il ne reste plus qu'à évaluer leur performance et choisir le modèle final. Une fois le modèle final déterminé, il ne restera plus qu'à faire une brève analyse de celui-ci avec une dernière évaluation de sa performance sur les données de validation, qui n'ont toujours pas été vues par le modèle.  


\subsection{Évaluation des performances}
Dans le but d'orienter notre choix de modèle, nous devons définir des mesures de performances qui permettront de comparer les modèles entre eux. \\

Dans les tâches de classification, les métriques que nous utilisons mettent l'emphase sur différents objectifs de classification. On pourrait vouloir par exemple mesurer la capacité du classifieur à prédire une bonne classe, la capacité de ne pas faire de fausses prédictions et/ou de ne jamais manquer de cas positifs. Ces mesures sont utilisées dans le cas binaire, où le classifier doit choisir soit une classe ou l'autre. Dans le  problème actuel, l'utilisation de 3 classes impose qu'une transformation vers plusieurs problèmes binaires soit faite afin de pouvoir calculer ces métriques. \\

Afin de transformer le problème de classification multiclasse en classification binaire, chaque classe sera prise indépendamment et sera confrontée aux 2 autres agrégées ensemble (méthode tous contre un). Cela revient à faire 3 classifications binaires. Une stratégie d'agrégation doit être mise en place pour faire un sommaire des statistiques des 3 classifieurs. \\ Une stratégie serait d'attribuer un poids identique à chacune des métriques générées par les 3 classifieurs binaires. Ce sont les métriques de type macro qu'on utilise lorsqu'une classe minoritaire doit être considérée aussi importante que les autres classes. \\ Une autre stratégie serait d'attribuer des poids proportionnels à la quantité de données appartenant à chacune des classes aux métriques générées par les 3 classifieurs binaires. Ce sont les métriques de type micro qu'on utilise lorsqu'aucune classe n'a plus d'importance qu'une autre - ou qu'il n'y a pas d'incitatif à prédire une classe plus qu'une autre. \\

Dans le cadre de ce problème, nous utiliserons les métriques de type micro car il n'y a pas de classe ayant besoin d'une attention accrue. Formellement, pour chaque classe $l$, les métriques utilisées sont: \\

- Exactitude (Taux de bonnes détections): l'exactitude est le pouvoir du classifieur à prédire la bonne classe pour une observation donnée.  Lors d'une classification, l'optimisation de cette métrique maximise le taux de bonne détection.\\
\begin{center}
	$\textrm{Exactitude} = \frac{\sum_{i=1}^{l} \frac{tp_i +tn_i}{tp_i + fn_i + fp_i+ tn_i}}{l}$
\end{center}

- Précision : la précision est la capacité d'un classifieur à ne trouver que les données d'une classe en particulier. Lors d'une classification, l'optimisation de cette métrique minimise les faux positifs.\\
\begin{center}
	$\textrm{Précision} = \frac{\sum_{i=1}^{l} tp_i}{\sum_{i=1}^{l} (tp_i + fp_i)}$
\end{center}

- Rappel (sensibilité): le rappel est la capacité du classifieur à bien trouver toutes les données d'une classe en particulier. Lors d'une classification, l'optimisation de cette métrique maximise les vrais positifs.\\
\begin{center}
	$\textrm{Rappel} = \frac{\sum_{i=1}^{l} tp_i}{\sum_{i=1}^{l} (tp_i + fn_i)}$
\end{center}

- Score F: le score F est la moyenne harmonique de la précision et du rappel. Elle sert de mesure agrégée sur laquelle prendre une décision. classe.\\	
\begin{center}
	$\textrm{Score F} = \frac{2 * \textrm{Précision} * \textrm{Rappel}}{\textrm{Précision} + \textrm{Rappel}}$ 
\end{center}

\subsection{Choix du modèle}

Dans le tableau \ref{tab:perfo_test}, nous présenterons l'exactitude et le score F pour chacun des classifieurs pour le jeu de données complet et à dimensionnalité réduite. Le choix du modèle final sera fait en considérant ces deux statistiques. \\

Comme le Score F agrège les concepts de précision et rappel, il a été choisi de ne pas présenter ces deux dernières. En effet, le but de notre classification est de donner la meilleure prédiction, peu importe la classe, et il n'y a pas d'erreur plus grave qu'une autre. L'exactitude ou le Score F1 sont deux métriques appropriées pour l'évaluation de notre classifieur. Dans le cas où il y aurait une très grande différence entre le Score F1 et l'exactitude, il pourrait être intéressant de bien comprendre d'où vient cet écart en analysant les ratios intermédiaires de précision et de rappel.

\rowcolors{2}{white}{white}\rowcolors{2}{gray!6}{white}
\begin{table}[H]
	
\caption{\label{tab:}Performances des différents modèles sur le jeu de données de test selon les différentes mesures de performance.}
\centering
\resizebox{\linewidth}{!}{\begin{tabular}[t]{lclcl}
		\hiderowcolors
		\toprule
		Modèles & Exactitude & F-Score & Exactitude avec PCA & F-Score avec PCA\\
		\midrule
		\showrowcolors
		Réseau de neurones & 0.7843 & 0.8382 & s/o & s/o\\
		Ensemble & 0.7614 & 0.8210 & 0.75 & 0.81 \\
		Multinomial & 0.7614 & 0.8210 & 0.75 & 0.81\\
		LDA & 0.7571 & 0.8178 & 0.75 & 0.81 \\
		Forêt aléatoire & 0.7555 & 0.8166 & s/o & s/o\\
		Arbre de décision & 0.7553 & 0.8165 & s/o & s/o\\
		Bayes & 0.7465 & 0.8099 & 0.74 & 0.80\\
		QDA & 0.7224 & 0.7918 & 0.74 & 0.80 \\
		SVM Gaussien & 0.7018 & 0.7763 & s/o & s/o\\
		\bottomrule
\end{tabular}}

\end{table}

En analysant les résultats ci-dessus, on remarque que le meilleur modèle est le réseau de neurones suivi de très près par des modèles beaucoup plus simple. Seulement 2\% séparent les deux métriques d'intérêt. On remarque aussi que les modèles. On rappelle ici que l'équipe désirait trouver le meilleur modèle en considérant une complexité appropriée au problème. Ainsi, malgré que le réseau de neurone soit un peu plus performant, nous choisirons le modèle multinomial, car il est beaucoup plus simple. De plus, il est possible que la différence notée dans les métriques de qualité soit due au hasard, supportant une fois de plus notre choix de modèle. 
 
\subsection{Présentation du modèle final}

Le modèle que nous avons choisi pour prédire les surfaces sur lesquelles les matchs de tennis ont été joués est le modèle multinomial. En utilisant un jeu de données qui avait été gardé secret tout au long du projet, ce modèle à une exactitude de 75.3\% et un score F1 de 81.5\%. La distribution des erreurs qu'il fait est présentée dans la matrice de confusion ci-dessous.

\rowcolors{2}{white}{white}
\rowcolors{2}{gray!6}{white}
\begin{table}[H]
	
	\caption{\label{tab:matrice_confusion}Matrice de confusion pour le modèle final.}
	\centering
	\begin{tabular}[t]{llcc}
		\hiderowcolors
		\toprule
		& Hard & Clay & Grass\\
		\midrule
		\showrowcolors
		Hard & 1495 & 558 & 307\\
		Clay & 279 & 536 & 34\\
		Grass & 17 & 10 & 32\\
		\bottomrule
	\end{tabular}
\end{table}

Comparativement à une attribution au hasard, ayant une exactitude de 57.2\%, il est possible d'affirmer qu'un apprentissage statistique a bel et bien eu lieu. 